{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA5vUeAjuLzK",
        "outputId": "f51a9e5a-cbfc-4945-b8d5-bbb972249aa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'hello': 0, 'there': 1, 'word': 2}\n",
            "[[1. 0. 1.]\n",
            " [1. 1. 0.]\n",
            " [3. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "#bow\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def build_vocabulary(corpus):\n",
        "  vocab = set()\n",
        "  for text in corpus:\n",
        "    vocab.update(text.split())\n",
        "  return {word: idx for idx, word in enumerate(sorted(vocab))}\n",
        "\n",
        "def bow_vectorize(corpus, vocab):\n",
        "  vectors = np.zeros((len(corpus), len(vocab)))\n",
        "  for i, text in enumerate(corpus):\n",
        "    for word in text.split():\n",
        "      if word in vocab:\n",
        "        vectors[i, vocab[word]] += 1\n",
        "  return vectors\n",
        "\n",
        "corpus_bow = [\"hello word\", \"hello there\", \"hello hello hello word\"]\n",
        "vocab_bow = build_vocabulary(corpus_bow)\n",
        "print(vocab_bow)\n",
        "bow_vectors = bow_vectorize(corpus_bow, vocab_bow)\n",
        "print(bow_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSFovrdISJg4",
        "outputId": "9ab78f48-8d44-4344-e1ca-22114da9af1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sci.space: 338 новин\n",
            "rec.autos: 347 новин\n",
            "talk.politics.mideast: 315 новин\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=['sci.space', 'rec.autos', 'talk.politics.mideast'], remove=('headers', 'footers', 'quotes'))\n",
        "corpus = newsgroups.data[:1000]\n",
        "labels = newsgroups.target[:1000]\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "category_counts = Counter(labels)\n",
        "category_names = {i: newsgroups.target_names[i] for i in category_counts}\n",
        "\n",
        "for category_id, count in category_counts.items():\n",
        "    print(f\"{category_names[category_id]}: {count} новин\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bAImgwWie3D",
        "outputId": "09ccc0ab-7a7a-4a92-e41b-bf191d3763f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "talk.politics.mideast: 315 новин\n",
            "rec.autos: 315 новин\n",
            "sci.space: 315 новин\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Мінімальна кількість новин серед усіх категорій\n",
        "min_count = min(category_counts.values())\n",
        "\n",
        "# Вибираємо рівну кількість новин для кожної категорії\n",
        "balanced_corpus = []\n",
        "balanced_labels = []\n",
        "\n",
        "for category_id in category_counts:\n",
        "    category_indices = [i for i, label in enumerate(labels) if label == category_id]\n",
        "    selected_indices = category_indices[:min_count]  # Вибираємо тільки min_count елементів\n",
        "\n",
        "    balanced_corpus.extend([corpus[i] for i in selected_indices])\n",
        "    balanced_labels.extend([labels[i] for i in selected_indices])\n",
        "\n",
        "# Перемішуємо, щоб уникнути порядкового впливу\n",
        "balanced_corpus, balanced_labels = shuffle(balanced_corpus, balanced_labels, random_state=42)\n",
        "\n",
        "# Нові збалансовані категорії\n",
        "new_counts = Counter(balanced_labels)\n",
        "for category_id, count in new_counts.items():\n",
        "    print(f\"{category_names[category_id]}: {count} новин\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGOPafbnikqA"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(balanced_corpus, balanced_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()  \n",
        "    text = re.sub(r'[^\\w\\s]', '', text) \n",
        "vocab = build_vocabulary([clean_text(text) for text in balanced_corpus])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2Wk46jtSTAC",
        "outputId": "1c4bec8f-7532-4e70-976c-9d312d7893e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BoW F1-score: 0.7071\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "X_train_bow = bow_vectorize(X_train, vocab)\n",
        "X_test_bow = bow_vectorize(X_test, vocab)\n",
        "\n",
        "clf_bow = LogisticRegression(max_iter=1000)\n",
        "clf_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "y_pred_bow = clf_bow.predict(X_test_bow)\n",
        "\n",
        "f1 = f1_score(y_test, y_pred_bow, average='weighted') \n",
        "print(f\"BoW F1-score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFLOdtCAy28Z"
      },
      "outputs": [],
      "source": [
        "#Tf-Idf\n",
        "\n",
        "def compute_tf(corpus, vocab):\n",
        "  tf_matrix = np.zeros((len(corpus), len(vocab)))\n",
        "  for i, text in enumerate(corpus):\n",
        "    words = text.split()\n",
        "    if len(words) == 0:\n",
        "      continue\n",
        "    for word in words:\n",
        "      if word in vocab:\n",
        "         tf_matrix[i, vocab[word]] += 1\n",
        "    tf_matrix[i] /= len(words)\n",
        "  return tf_matrix\n",
        "\n",
        "def compute_idf(corpus, vocab):\n",
        "    doc_count = len(corpus)\n",
        "    idf = np.zeros(len(vocab))  \n",
        "    for word, idx in vocab.items():\n",
        "        df = sum(1 for text in corpus if word in text.split())\n",
        "        idf[idx] = np.log(doc_count / (df + 1)) \n",
        "    return idf\n",
        "\n",
        "def compute_tfidf(corpus, vocab):\n",
        "  tf_matrix = compute_tf(corpus, vocab)\n",
        "  idx_vector = compute_idf(corpus, vocab)\n",
        "\n",
        "  return tf_matrix * idx_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx5e2gRCSacV",
        "outputId": "b6139727-fafd-48b3-935a-30058d369e0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BoW F1-score: 0.7597\n"
          ]
        }
      ],
      "source": [
        "tfidf_vectors_train = compute_tfidf(X_train, vocab)\n",
        "tfidf_vectors_test = compute_tfidf(X_test, vocab)\n",
        "\n",
        "clf_tfidf = LogisticRegression(max_iter=1000)\n",
        "clf_tfidf.fit(tfidf_vectors_train, y_train)\n",
        "\n",
        "y_pred_tfidf = clf_tfidf.predict(tfidf_vectors_test)\n",
        "\n",
        "f1 = f1_score(y_test, y_pred_tfidf, average='weighted')  \n",
        "print(f\"BoW F1-score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Qxk_7fk142D"
      },
      "outputs": [],
      "source": [
        "#Word2Vec (Skip-Gram)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "class SkipGramModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(SkipGramModel, self).__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "  def forward(self, center_word):\n",
        "    embed = self.embeddings(center_word)\n",
        "    output = self.output_layer(embed)\n",
        "    return output\n",
        "\n",
        "def generate_skip_gram_pairs(corpus, vocab, window_size=2):\n",
        "    pairs = []\n",
        "    for text in corpus:\n",
        "        words = text.split()\n",
        "        for i, center in enumerate(words):\n",
        "            if center not in vocab:\n",
        "                continue\n",
        "            context_words = words[max(0, i-window_size):i] + words[i+1:i+1+window_size]\n",
        "            pairs.extend([(vocab[center], vocab[w]) for w in context_words if w in vocab])\n",
        "    return pairs\n",
        "\n",
        "def create_batches(pairs, batch_size):\n",
        "    random.shuffle(pairs)\n",
        "    batches = [pairs[i:i+batch_size] for i in range(0, len(pairs), batch_size)]\n",
        "    return batches\n",
        "\n",
        "# Навчання моделі\n",
        "def train_skip_gram(corpus, vocab, embedding_dim=100, epochs=50, lr=0.01, batch_size=256):\n",
        "    pairs = generate_skip_gram_pairs(corpus, vocab)\n",
        "    batches = create_batches(pairs, batch_size)\n",
        "    model = SkipGramModel(len(vocab), embedding_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in batches:\n",
        "            center_words = torch.tensor([pair[0] for pair in batch], dtype=torch.long)\n",
        "            context_words = torch.tensor([pair[1] for pair in batch], dtype=torch.long)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(center_words)\n",
        "            loss = loss_function(output, context_words)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        average_loss = total_loss / len(batches)\n",
        "\n",
        "        print(f\"Epoch {epoch}, Loss: {average_loss:.4f}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtPcFSZD-p75",
        "outputId": "c4c642b0-47cc-4f70-8471-59f0cc000b30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 7.0003\n",
            "Epoch 1, Loss: 6.3559\n",
            "Epoch 2, Loss: 6.1200\n",
            "Epoch 3, Loss: 5.9846\n",
            "Epoch 4, Loss: 5.9055\n",
            "Epoch 5, Loss: 5.8549\n",
            "Epoch 6, Loss: 5.8240\n",
            "Epoch 7, Loss: 5.8020\n",
            "Epoch 8, Loss: 5.7875\n",
            "Epoch 9, Loss: 5.7765\n",
            "Epoch 10, Loss: 5.7683\n",
            "Epoch 11, Loss: 5.7619\n",
            "Epoch 12, Loss: 5.7574\n",
            "Epoch 13, Loss: 5.7535\n",
            "Epoch 14, Loss: 5.7506\n",
            "Epoch 15, Loss: 5.7474\n",
            "Epoch 16, Loss: 5.7459\n",
            "Epoch 17, Loss: 5.7440\n",
            "Epoch 18, Loss: 5.7427\n",
            "Epoch 19, Loss: 5.7421\n",
            "Epoch 20, Loss: 5.7411\n",
            "Epoch 21, Loss: 5.7396\n",
            "Epoch 22, Loss: 5.7391\n",
            "Epoch 23, Loss: 5.7386\n",
            "Epoch 24, Loss: 5.7383\n",
            "Epoch 25, Loss: 5.7379\n",
            "Epoch 26, Loss: 5.7375\n",
            "Epoch 27, Loss: 5.7371\n",
            "Epoch 28, Loss: 5.7364\n",
            "Epoch 29, Loss: 5.7359\n",
            "Epoch 30, Loss: 5.7352\n",
            "Epoch 31, Loss: 5.7349\n",
            "Epoch 32, Loss: 5.7346\n",
            "Epoch 33, Loss: 5.7344\n",
            "Epoch 34, Loss: 5.7341\n",
            "Epoch 35, Loss: 5.7337\n",
            "Epoch 36, Loss: 5.7338\n",
            "Epoch 37, Loss: 5.7341\n",
            "Epoch 38, Loss: 5.7325\n",
            "Epoch 39, Loss: 5.7328\n",
            "Epoch 40, Loss: 5.7329\n",
            "Epoch 41, Loss: 5.7331\n",
            "Epoch 42, Loss: 5.7321\n",
            "Epoch 43, Loss: 5.7332\n",
            "Epoch 44, Loss: 5.7319\n",
            "Epoch 45, Loss: 5.7326\n",
            "Epoch 46, Loss: 5.7314\n",
            "Epoch 47, Loss: 5.7327\n",
            "Epoch 48, Loss: 5.7317\n",
            "Epoch 49, Loss: 5.7314\n"
          ]
        }
      ],
      "source": [
        "skip_gram_train = train_skip_gram(balanced_corpus, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wacvIG_elLYu",
        "outputId": "24b0222c-9208-4c33-8e89-be928affbbb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BoW F1-score: 0.7253\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Функція для отримання середнього векторного представлення тексту\n",
        "def get_text_embedding(text, model, vocab):\n",
        "    words = text.split()\n",
        "    vectors = [model.embeddings.weight[vocab[word]].detach().numpy() for word in words if word in vocab]\n",
        "    if len(vectors) > 0:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.embeddings.embedding_dim)\n",
        "\n",
        "X_train_vectors = np.array([get_text_embedding(clean_text(text), skip_gram_train, vocab) for text in X_train])\n",
        "X_test_vectors = np.array([get_text_embedding(clean_text(text), skip_gram_train, vocab) for text in X_test])\n",
        "\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train_vectors, y_train)\n",
        "\n",
        "# Оцінка моделі\n",
        "y_pred = classifier.predict(X_test_vectors)\n",
        "\n",
        "f1 = f1_score(y_test, y_pred, average='weighted') \n",
        "print(f\"BoW F1-score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ByURODY5lfD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import re\n",
        "\n",
        "#Word2Vec (CBOW)\n",
        "\n",
        "class CBOWModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(CBOWModel, self).__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "  def forward(self, center_word):\n",
        "    embeds = self.embeddings(center_word)\n",
        "    avg_embeds = torch.mean(embeds, dim=1)\n",
        "    output = self.linear(avg_embeds)\n",
        "    return output\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Lowercases text and removes punctuation.\"\"\"\n",
        "    return re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "\n",
        "def generate_cbow_pairs(corpus, vocab, window_size=2):\n",
        "    pairs = []\n",
        "    for text in corpus:\n",
        "        words = preprocess_text(text).split()\n",
        "        for i, center in enumerate(words):\n",
        "            context_words = words[max(0, i-window_size):i] + words[i+1:i+1+window_size]\n",
        "            if len(context_words) == 2 * window_size:\n",
        "              pairs.append(([vocab[w] for w in context_words if w in vocab], vocab[center]))\n",
        "    return pairs\n",
        "\n",
        "def create_batches(pairs, batch_size):\n",
        "    random.shuffle(pairs)\n",
        "    batches = [pairs[i:i+batch_size] for i in range(0, len(pairs), batch_size)]\n",
        "    return batches\n",
        "\n",
        "# Навчання моделі\n",
        "def train_cbow(corpus, vocab, embedding_dim=100, epochs=50, lr=0.01, batch_size=256):\n",
        "    pairs = generate_cbow_pairs(corpus, vocab)\n",
        "    batches = create_batches(pairs, batch_size)\n",
        "    model = CBOWModel(len(vocab), embedding_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in batches:\n",
        "            center_words = torch.tensor([pair[0] for pair in batch], dtype=torch.long)\n",
        "            context_words = torch.tensor([pair[1] for pair in batch], dtype=torch.long)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(center_words)\n",
        "            loss = loss_function(output, context_words)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        average_loss = total_loss / len(batches)\n",
        "\n",
        "        print(f\"Epoch {epoch}, Loss: {average_loss:.4f}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbBkFlC1QnCF",
        "outputId": "40f8cd7a-33dd-40ae-cce5-050ad692e1ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 7.1699\n",
            "Epoch 1, Loss: 5.3017\n",
            "Epoch 2, Loss: 4.1452\n",
            "Epoch 3, Loss: 3.3306\n",
            "Epoch 4, Loss: 2.7977\n",
            "Epoch 5, Loss: 2.4532\n",
            "Epoch 6, Loss: 2.2201\n",
            "Epoch 7, Loss: 2.0573\n",
            "Epoch 8, Loss: 1.9355\n",
            "Epoch 9, Loss: 1.8436\n",
            "Epoch 10, Loss: 1.7687\n",
            "Epoch 11, Loss: 1.7082\n",
            "Epoch 12, Loss: 1.6585\n",
            "Epoch 13, Loss: 1.6128\n",
            "Epoch 14, Loss: 1.5738\n",
            "Epoch 15, Loss: 1.5409\n",
            "Epoch 16, Loss: 1.5134\n",
            "Epoch 17, Loss: 1.4873\n",
            "Epoch 18, Loss: 1.4641\n",
            "Epoch 19, Loss: 1.4451\n",
            "Epoch 20, Loss: 1.4252\n",
            "Epoch 21, Loss: 1.4102\n",
            "Epoch 22, Loss: 1.3946\n",
            "Epoch 23, Loss: 1.3825\n",
            "Epoch 24, Loss: 1.3703\n",
            "Epoch 25, Loss: 1.3562\n",
            "Epoch 26, Loss: 1.3427\n",
            "Epoch 27, Loss: 1.3359\n",
            "Epoch 28, Loss: 1.3268\n",
            "Epoch 29, Loss: 1.3207\n",
            "Epoch 30, Loss: 1.3128\n",
            "Epoch 31, Loss: 1.3019\n",
            "Epoch 32, Loss: 1.2935\n",
            "Epoch 33, Loss: 1.2861\n",
            "Epoch 34, Loss: 1.2836\n",
            "Epoch 35, Loss: 1.2756\n",
            "Epoch 36, Loss: 1.2700\n",
            "Epoch 37, Loss: 1.2638\n",
            "Epoch 38, Loss: 1.2609\n",
            "Epoch 39, Loss: 1.2538\n",
            "Epoch 40, Loss: 1.2517\n",
            "Epoch 41, Loss: 1.2474\n",
            "Epoch 42, Loss: 1.2424\n",
            "Epoch 43, Loss: 1.2384\n",
            "Epoch 44, Loss: 1.2334\n",
            "Epoch 45, Loss: 1.2294\n",
            "Epoch 46, Loss: 1.2284\n",
            "Epoch 47, Loss: 1.2227\n",
            "Epoch 48, Loss: 1.2211\n",
            "Epoch 49, Loss: 1.2183\n"
          ]
        }
      ],
      "source": [
        "cbow_train = train_cbow(balanced_corpus, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYUK7ybUn3vr",
        "outputId": "811f9f4d-3f28-424b-8234-bfce5c14a88e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CBOW F1-score: 0.7829\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def get_text_embedding(text, model, vocab):\n",
        "    words = text.split()\n",
        "    vectors = [model.embeddings.weight[vocab[word]].detach().numpy() for word in words if word in vocab]\n",
        "    if len(vectors) > 0:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.embeddings.embedding_dim)\n",
        "\n",
        "X_train_vectors = np.array([get_text_embedding(clean_text(text), cbow_train, vocab) for text in X_train])\n",
        "X_test_vectors = np.array([get_text_embedding(clean_text(text), cbow_train, vocab) for text in X_test])\n",
        "\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train_vectors, y_train)\n",
        "\n",
        "# Оцінка моделі\n",
        "y_pred_cbow = classifier.predict(X_test_vectors)\n",
        "\n",
        "f1 = f1_score(y_test, y_pred_cbow, average='weighted') \n",
        "print(f\"CBOW F1-score: {f1:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
